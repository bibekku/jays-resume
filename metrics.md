| **Use Case**                        | **Operational Metrics**                                                                                                                                          | **Content/Quality Metrics**                                                                                                                   |
| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| **Agentic Workflows**               | • Task latency (end-to-end)  <br> • Span-level latency per agent call <br> • A2A error/failure rate <br> • Multi-hop agent count <br> • Resource usage per agent | • Task success rate <br> • Agent rationale coherence (LLM-as-judge) <br> • Conflict resolution frequency & latency                            |
| **DocSum (Summarization)**          | • Latency per document <br> • Token cost (in + out) <br> • Error rate (due to truncation/hallucination)                                                          | • ROUGE‑1/2/L metrics <br> • Coverage (% of topics included) <br> • Fluency/coherence (LLM-as-judge scores)                                   |
| **CodeGen**                         | • Generation latency <br> • Token cost <br> • API failure rate                                                                                                   | • Compilation success rate (%) <br> • Unit-test pass rate (%) <br> • Syntax error rate <br> • Semantic correctness score (LLM-judge or human) |
| **IVA (Internal Assistant)**        | • Response latency (p50/p95/p99) <br> • Feedback rate (e.g., thumbs-up/down) <br> • Re-ask count per session <br> • Session length (# turns, duration)           | • Intent recognition accuracy <br> • Resolution rate (%) <br> • Relevance/helpfulness (LLM-judge)                                             |
| **DocCreate (Document Generation)** | • Generation latency <br> • Token cost <br> • % outputs revised by user <br> • Error/regeneration rate                                                           | • Coverage completeness (LLM-judge) <br> • Structural correctness (presence/order of required sections) <br> • Factual consistency score      |
| **Cross-Use-Case Metrics**          | • Average token cost per request <br> • Hallucination rate (%) <br> • Error/failure rate overall <br> • Toxicity/PII flag rate                                   | • Quality drift over time (LLM-judge trajectories) <br> • Aggregated user satisfaction (feedback score)                                       |
